{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Parking Tickets: An Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: To execute the code of this file the Kernel should be selected as PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing SparkSession Module\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creating SparkSession object\n",
    "spark = SparkSession.builder.appName(\"NYC Parking Tickets\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "nyc_pt = spark.read.csv(\"/common_folder/nyc_parking/Parking_Violations_Issued_-_Fiscal_Year_2017.csv\",\n",
    "                        header=True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 10803028 \n",
      " Number of Columns: 10\n"
     ]
    }
   ],
   "source": [
    "# Number of rows and column in dataset\n",
    "print(\"Number of rows:\",nyc_pt.count(),\"\\n\",\"Number of Columns:\",len(nyc_pt.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Summons Number: long (nullable = true)\n",
      " |-- Plate ID: string (nullable = true)\n",
      " |-- Registration State: string (nullable = true)\n",
      " |-- Issue Date: timestamp (nullable = true)\n",
      " |-- Violation Code: integer (nullable = true)\n",
      " |-- Vehicle Body Type: string (nullable = true)\n",
      " |-- Vehicle Make: string (nullable = true)\n",
      " |-- Violation Precinct: integer (nullable = true)\n",
      " |-- Issuer Precinct: integer (nullable = true)\n",
      " |-- Violation Time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspecting columns in dataset\n",
    "nyc_pt.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns for better handling of columns\n",
    "nyc_pt = nyc_pt.withColumnRenamed(\"Summons Number\", \"Summons_Number\")\n",
    "nyc_pt = nyc_pt.withColumnRenamed(\"Plate ID\", \"Plate_ID\")\n",
    "nyc_pt = nyc_pt.withColumnRenamed(\"Registration State\", \"Registration_State\")\n",
    "nyc_pt = nyc_pt.withColumnRenamed(\"Issue Date\", \"Issue_Date\")\n",
    "nyc_pt = nyc_pt.withColumnRenamed(\"Violation Code\", \"Violation_Code\")\n",
    "nyc_pt = nyc_pt.withColumnRenamed(\"Vehicle Body Type\", \"Vehicle_Body_Type\")\n",
    "nyc_pt = nyc_pt.withColumnRenamed(\"Vehicle Make\", \"Vehicle_Make\")\n",
    "nyc_pt = nyc_pt.withColumnRenamed(\"Violation Precinct\", \"Violation_Precinct\")\n",
    "nyc_pt = nyc_pt.withColumnRenamed(\"Issuer Precinct\", \"Issuer_Precinct\")\n",
    "nyc_pt = nyc_pt.withColumnRenamed(\"Violation Time\", \"Violation_Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|Summons_Number|Plate_ID|Registration_State|         Issue_Date|Violation_Code|Vehicle_Body_Type|Vehicle_Make|Violation_Precinct|Issuer_Precinct|Violation_Time|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|    5092469481| GZH7067|                NY|2016-07-10 00:00:00|             7|             SUBN|       TOYOT|                 0|              0|         0143A|\n",
      "|    5092451658| GZH7067|                NY|2016-07-08 00:00:00|             7|             SUBN|       TOYOT|                 0|              0|         0400P|\n",
      "|    4006265037| FZX9232|                NY|2016-08-23 00:00:00|             5|             SUBN|        FORD|                 0|              0|         0233P|\n",
      "|    8478629828| 66623ME|                NY|2017-06-14 00:00:00|            47|             REFG|       MITSU|                14|             14|         1120A|\n",
      "|    7868300310| 37033JV|                NY|2016-11-21 00:00:00|            69|             DELV|       INTER|                13|             13|         0555P|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Again inspecting after renaming columns to verify rename operation is done successfully\n",
    "nyc_pt.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all functions from pyspark\n",
    "import pyspark.sql.functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Summons_Number', 0),\n",
       " ('Plate_ID', 0),\n",
       " ('Registration_State', 0),\n",
       " ('Issue_Date', 0),\n",
       " ('Violation_Code', 0),\n",
       " ('Vehicle_Body_Type', 0),\n",
       " ('Vehicle_Make', 0),\n",
       " ('Violation_Precinct', 0),\n",
       " ('Issuer_Precinct', 0),\n",
       " ('Violation_Time', 0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting missing value for each column\n",
    "missing_per_col = []\n",
    "for c in nyc_pt.columns:\n",
    "    missing_per_col.append(nyc_pt.filter(func.isnull(nyc_pt[c])).count())\n",
    "    \n",
    "cols_missings = list(zip(nyc_pt.columns, missing_per_col))\n",
    "cols_missings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Issue_Date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|    min(Issue_Date)|    max(Issue_Date)|\n",
      "+-------------------+-------------------+\n",
      "|1972-03-30 00:00:00|2069-11-19 00:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspecting range of Issue date\n",
    "nyc_pt.select(func.min(\"Issue_Date\"), func.max(\"Issue_Date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|year(Issue_Date)|\n",
      "+----------------+\n",
      "|            1990|\n",
      "|            2025|\n",
      "|            1977|\n",
      "|            2027|\n",
      "|            2003|\n",
      "|            2007|\n",
      "|            2018|\n",
      "|            1974|\n",
      "|            2015|\n",
      "|            2023|\n",
      "+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Different year of Issue_Date\n",
    "nyc_pt.select(func.year(\"Issue_Date\")).distinct().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|Summons_Number|Plate_ID|Registration_State|         Issue_Date|Violation_Code|Vehicle_Body_Type|Vehicle_Make|Violation_Precinct|Issuer_Precinct|Violation_Time|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|    5092469481| GZH7067|                NY|2016-07-10 00:00:00|             7|             SUBN|       TOYOT|                 0|              0|         0143A|\n",
      "|    5092451658| GZH7067|                NY|2016-07-08 00:00:00|             7|             SUBN|       TOYOT|                 0|              0|         0400P|\n",
      "|    4006265037| FZX9232|                NY|2016-08-23 00:00:00|             5|             SUBN|        FORD|                 0|              0|         0233P|\n",
      "|    8478629828| 66623ME|                NY|2017-06-14 00:00:00|            47|             REFG|       MITSU|                14|             14|         1120A|\n",
      "|    7868300310| 37033JV|                NY|2016-11-21 00:00:00|            69|             DELV|       INTER|                13|             13|         0555P|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Creating view of the dataframe to perform Sql operation on it\n",
    "nyc_pt.createOrReplaceTempView(\"base_nyc_table\")\n",
    "\n",
    "# Inspecting View\n",
    "spark.sql(\"select * from base_nyc_table\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">1. Find the total number of tickets for the year.</span>\n",
    "\n",
    "#### Assumption: \n",
    "Dataset contains data from the year 2016 and 2017 i.e. financial year 2017.\n",
    "\n",
    "For Analysis, we will consider whole data i.e. __data for year 2016 and 2017.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|ticket_freq|\n",
      "+-----------+\n",
      "|   10803028|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Counting total number of tickets for financial year 2017\n",
    "ticket_freq_2017 = spark.sql(\"select count(*) as ticket_freq \\\n",
    "                              from base_nyc_table\")\n",
    "ticket_freq_2017.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">2. Find out the number of unique states from where the cars that got parking tickets came.</span>\n",
    "__(Hint: Use the column 'Registration State'.)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|Unique_State_Count|\n",
      "+------------------+\n",
      "|                67|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Counting the number of unique states from where cars got parking tickets\n",
    "unique_states_freq = spark.sql(\"select count(distinct Registration_State) as Unique_State_Count\\\n",
    "                                from base_nyc_table\")\n",
    "unique_states_freq.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+\n",
      "|Registration_State|  Count|\n",
      "+------------------+-------+\n",
      "|                NY|8481061|\n",
      "|                NJ| 925965|\n",
      "|                PA| 285419|\n",
      "|                FL| 144556|\n",
      "|                CT| 141088|\n",
      "|                MA|  85547|\n",
      "|                IN|  80749|\n",
      "|                VA|  72626|\n",
      "|                MD|  61800|\n",
      "|                NC|  55806|\n",
      "|                IL|  37329|\n",
      "|                GA|  36852|\n",
      "|                99|  36625|\n",
      "|                TX|  36516|\n",
      "|                AZ|  26426|\n",
      "|                OH|  25302|\n",
      "|                CA|  24260|\n",
      "|                SC|  21836|\n",
      "|                ME|  21574|\n",
      "|                MN|  18227|\n",
      "+------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspecting unique states and identifying the number of parking tickets issued by each states\n",
    "unique_states = spark.sql(\"select Registration_State, count(*) as Count\\\n",
    "                           from base_nyc_table\\\n",
    "                           group by Registration_State\\\n",
    "                           order by Count desc\")\n",
    "unique_states.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a __numeric entry '99'__ in the column, which should be corrected.<br>\n",
    "\n",
    "Replacing it with the state having the maximum entries i.e. __replacing '99' with 'NY' which is having maximum entries.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing '99' with 'NY'\n",
    "nyc_pt_final = nyc_pt.withColumn(\"Registration_State\", func.when(func.col(\"Registration_State\")==99, 'NY')\\\n",
    "                           .otherwise(func.col(\"Registration_State\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Updating View\n",
    "nyc_pt_final.createOrReplaceTempView(\"base_nyc_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|Unique_State_Count|\n",
      "+------------------+\n",
      "|                66|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Again counting the number of unique states from where cars got parking tickets\n",
    "corrected_unique_states_freq = spark.sql(\"select count(distinct Registration_State) as Unique_State_Count\\\n",
    "                                         from base_nyc_table\")\n",
    "corrected_unique_states_freq.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The number of unique states from where the cars that got parking tickets are <span style=\"color:blue\">66</span>__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">1. How often does each violation code occur? Display the frequency of the top five violation codes.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+\n",
      "|Violation_Code|  Count|\n",
      "+--------------+-------+\n",
      "|            21|1528588|\n",
      "|            36|1400614|\n",
      "|            38|1062304|\n",
      "|            14| 893498|\n",
      "|            20| 618593|\n",
      "+--------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding frequency(count) of violation code\n",
    "violation_code_freq = spark.sql(\"select Violation_Code, count(*) as Count \\\n",
    "                                 from base_nyc_table\\\n",
    "                                 group by Violation_Code\\\n",
    "                                 order by Count desc\")\n",
    "\n",
    "violation_code_freq.show(5) # Displaying top five violation codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">2. How often does each 'vehicle body type' get a parking ticket? How about the 'vehicle make'?</span>\n",
    "__(Hint: Find the top 5 for both.)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+\n",
      "|Vehicle_Body_Type|  Count|\n",
      "+-----------------+-------+\n",
      "|             SUBN|3719802|\n",
      "|             4DSD|3082020|\n",
      "|              VAN|1411970|\n",
      "|             DELV| 687330|\n",
      "|              SDN| 438191|\n",
      "+-----------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding frequency(count) of parking ticket for vehicle body type\n",
    "vehicle_body_type_freq = spark.sql(\"select Vehicle_Body_Type, count(*) as Count \\\n",
    "                                    from base_nyc_table\\\n",
    "                                    group by Vehicle_Body_Type\\\n",
    "                                    order by Count desc\")\n",
    "\n",
    "vehicle_body_type_freq.show(5) # Displaying top five frequency for vehicle body type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n",
      "|Vehicle_Make|  Count|\n",
      "+------------+-------+\n",
      "|        FORD|1280958|\n",
      "|       TOYOT|1211451|\n",
      "|       HONDA|1079238|\n",
      "|       NISSA| 918590|\n",
      "|       CHEVR| 714655|\n",
      "+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding frequency(count) of parking ticket for vehicle make\n",
    "vehicle_make_freq = spark.sql(\"select Vehicle_Make, count(*) as Count \\\n",
    "                               from base_nyc_table\\\n",
    "                               group by Vehicle_Make\\\n",
    "                               order by Count desc\")\n",
    "\n",
    "vehicle_make_freq.show(5) # Displaying top five frequency for vehicle make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">3. A precinct is a police station that has a certain zone of the city under its command. Find the (5 highest) frequencies of tickets for each of the following:</span>\n",
    "__(Hint: Print the top six entries after sorting.)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">3.1 'Violation Precinct' (This is the precinct of the zone where the violation occurred).</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+\n",
      "|Violation_Precinct|  Count|\n",
      "+------------------+-------+\n",
      "|                 0|2072400|\n",
      "|                19| 535671|\n",
      "|                14| 352450|\n",
      "|                 1| 331810|\n",
      "|                18| 306920|\n",
      "|               114| 296514|\n",
      "+------------------+-------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding frequency(count) of tickets for Violation Precinct\n",
    "violation_precinct_freq = spark.sql(\"select Violation_Precinct, count(*) as Count \\\n",
    "                                     from base_nyc_table\\\n",
    "                                     group by Violation_Precinct\\\n",
    "                                     order by Count desc\")\n",
    "\n",
    "violation_precinct_freq.show(6) # Displaying top 6 frequencies for Violation Precinct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">Using this, can you draw any insights for parking violations in any specific areas of the city?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+-------+\n",
      "|Registration_State|Violation_Precinct|  Count|\n",
      "+------------------+------------------+-------+\n",
      "|                NY|                 0|1749299|\n",
      "|                NY|                19| 416114|\n",
      "|                NY|               114| 249486|\n",
      "|                NY|                 1| 236237|\n",
      "|                NY|                14| 232881|\n",
      "|                NY|                18| 200594|\n",
      "|                NY|               109| 189639|\n",
      "|                NY|                13| 172687|\n",
      "|                NY|                70| 155195|\n",
      "|                NY|               115| 143762|\n",
      "|                NY|                61| 138470|\n",
      "|                NY|                84| 134061|\n",
      "|                NY|                17| 131612|\n",
      "|                NY|               108| 130819|\n",
      "|                NY|               112| 130674|\n",
      "|                NY|                66| 129789|\n",
      "|                NY|               103| 121269|\n",
      "|                NY|                52| 119926|\n",
      "|                NY|                20| 119846|\n",
      "|                NJ|                 0| 117670|\n",
      "+------------------+------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vio_pre_freq_area = spark.sql(\"select Registration_State, Violation_Precinct, count(*) as Count \\\n",
    "                               from base_nyc_table\\\n",
    "                               group by Registration_State, Violation_Precinct\\\n",
    "                               order by Count desc\")\n",
    "vio_pre_freq_area.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">3.2 'Issuer Precinct' (This is the precinct that issued the ticket.)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|Issuer_Precinct|  Count|\n",
      "+---------------+-------+\n",
      "|              0|2388479|\n",
      "|             19| 521513|\n",
      "|             14| 344977|\n",
      "|              1| 321170|\n",
      "|             18| 296553|\n",
      "|            114| 289950|\n",
      "+---------------+-------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding frequency(count) of tickets for Issuer Precinct\n",
    "issuer_precinct_freq = spark.sql(\"select Issuer_Precinct, count(*) as Count \\\n",
    "                                  from base_nyc_table\\\n",
    "                                  group by Issuer_Precinct\\\n",
    "                                  order by Count desc\")\n",
    "issuer_precinct_freq.show(6) # Displaying top 6 frequencies for Issuer Precinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+-------+\n",
      "|Registration_State|Issuer_Precinct|  Count|\n",
      "+------------------+---------------+-------+\n",
      "|                NY|              0|1974434|\n",
      "|                NY|             19| 405074|\n",
      "|                NY|            114| 244046|\n",
      "|                NY|              1| 228990|\n",
      "|                NY|             14| 227876|\n",
      "|                NY|             18| 192930|\n",
      "|                NY|            109| 190153|\n",
      "|                NY|             13| 168632|\n",
      "|                NY|             70| 148760|\n",
      "|                NJ|              0| 142846|\n",
      "|                NY|            115| 139625|\n",
      "|                NY|             61| 132406|\n",
      "|                NY|             84| 129055|\n",
      "|                NY|             17| 128809|\n",
      "|                NY|            112| 126026|\n",
      "|                NY|            108| 125469|\n",
      "|                NY|             66| 119597|\n",
      "|                NY|            103| 118662|\n",
      "|                NY|             20| 118045|\n",
      "|                NY|             52| 116590|\n",
      "+------------------+---------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "issuer_pre_freq_area = spark.sql(\"select Registration_State, Issuer_Precinct, count(*) as Count \\\n",
    "                                  from base_nyc_table\\\n",
    "                                  group by Registration_State, Issuer_Precinct\\\n",
    "                                  order by Count desc\")\n",
    "\n",
    "issuer_pre_freq_area.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">4 Find the violation code frequencies for three precincts that have issued the most number of tickets. Do these precinct zones have an exceptionally high frequency of certain violation codes? Are these codes common across precincts?</span><br>\n",
    "__(Hint: In the SQL view, use the 'where' attribute to filter among three precincts.)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">4.1 Find the violation code frequencies for three precincts that have issued the most number of tickets.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+\n",
      "|Issuer_Precinct| Count|\n",
      "+---------------+------+\n",
      "|             19|521513|\n",
      "|             14|344977|\n",
      "|              1|321170|\n",
      "+---------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding top 3 precincts that have issued most number of tickets filtering erroneous entry of '0'\n",
    "top3_precincts = spark.sql(\"select Issuer_Precinct,count(*) as Count\\\n",
    "                            from base_nyc_table\\\n",
    "                            where Issuer_Precinct != 0\\\n",
    "                            group by Issuer_Precinct\\\n",
    "                            order by Count desc\")\n",
    "\n",
    "top3_precincts.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+-----+\n",
      "|Issuer_Precinct|Violation_Code|Count|\n",
      "+---------------+--------------+-----+\n",
      "|             19|            46|86390|\n",
      "|             19|            37|72437|\n",
      "|             19|            38|72344|\n",
      "|             19|            14|57563|\n",
      "|             19|            21|54700|\n",
      "+---------------+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Violation code frequency for highest Issuer precincts i.e. 19\n",
    "top3_precincts_19 = spark.sql(\"select Issuer_Precinct,Violation_Code, count(*) as Count\\\n",
    "                               from base_nyc_table\\\n",
    "                               where Issuer_Precinct = 19\\\n",
    "                               group by Issuer_Precinct,Violation_Code\\\n",
    "                               order by Count desc\")\n",
    "\n",
    "top3_precincts_19.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+-----+\n",
      "|Issuer_Precinct|Violation_Code|Count|\n",
      "+---------------+--------------+-----+\n",
      "|             14|            14|73837|\n",
      "|             14|            69|58026|\n",
      "|             14|            31|39857|\n",
      "|             14|            47|30540|\n",
      "|             14|            42|20663|\n",
      "+---------------+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Violation code frequency for second highest Issuer precincts i.e. 14\n",
    "top3_precincts_14 = spark.sql('select Issuer_Precinct,Violation_Code, count(*) as Count\\\n",
    "                               from base_nyc_table\\\n",
    "                               where Issuer_Precinct = 14\\\n",
    "                               group by Issuer_Precinct,Violation_Code\\\n",
    "                               order by Count desc')\n",
    "\n",
    "top3_precincts_14.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+-----+\n",
      "|Issuer_Precinct|Violation_Code|Count|\n",
      "+---------------+--------------+-----+\n",
      "|              1|            14|73522|\n",
      "|              1|            16|38937|\n",
      "|              1|            20|27841|\n",
      "|              1|            46|22534|\n",
      "|              1|            38|16989|\n",
      "+---------------+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Violation code frequency for third highest Issuer precincts i.e. 1\n",
    "top3_precincts_1 = spark.sql('select Issuer_Precinct,Violation_Code, count(*) as Count\\\n",
    "                               from base_nyc_table\\\n",
    "                               where Issuer_Precinct = 1\\\n",
    "                               group by Issuer_Precinct,Violation_Code\\\n",
    "                               order by Count desc')\n",
    "\n",
    "top3_precincts_1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">4.2 Do these precinct zones have an exceptionally high frequency of certain violation codes?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For Issuer_Precinct - 19\n",
    "    - Top five violation code are\n",
    "            46|86390|\n",
    "            37|72437|\n",
    "            38|72344|\n",
    "            14|57563|\n",
    "            21|54700|\n",
    "    - We can see for violation code 46, frequency is on higher side as compared to other violation code.\n",
    "\n",
    "- For Issuer_Precinct - 14\n",
    "    - Top five violation code are\n",
    "            14|73837|\n",
    "            69|58026|\n",
    "            31|39857|\n",
    "            47|30540|\n",
    "            42|20663|\n",
    "    - We can see for violation code 14, frequency is on higher side as compared to other violation code.\n",
    "\n",
    "- For Issuer_Precinct - 1\n",
    "    - Top five violation code are\n",
    "            14|73522|\n",
    "            16|38937|\n",
    "            20|27841|\n",
    "            46|22534|\n",
    "            38|16989|\n",
    "    - We can see for violation code 1, frequency is exceptionally high as compared to other violation code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">4.3 Are these codes common across precincts?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">Assumption</span>\n",
    "To answer this we are considering top 5 violation code for top three precincts\n",
    "\n",
    "#### <span style=\"color:blue\">Conclusion</span>\n",
    "From the above summary, Considering top 5 violation code for top three precincts that have issued the most number of tickets, \n",
    "\n",
    "we can observe that __violation code 14 is common across top three precincts__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">5. Find out the properties of parking violations across different times of the day:</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">5.1 Find a way to deal with missing values, if any.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------+\n",
      "|count(CASE WHEN (isnan(Violation_Time) OR (Violation_Time IS NULL)) THEN Violation_Time END)|\n",
      "+--------------------------------------------------------------------------------------------+\n",
      "|                                                                                           0|\n",
      "+--------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing specific functions to count nan\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "# Inspecting Null and NaN values in Violation_Time\n",
    "nyc_pt_final.select([count(when(isnan(\"Violation_Time\") | col(\"Violation_Time\").isNull(), \"Violation_Time\"))]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count nan in Violation_Time 63\n"
     ]
    }
   ],
   "source": [
    "print(\"Count nan in Violation_Time\",nyc_pt_final.filter(nyc_pt_final.Violation_Time == 'nan').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing value can be of two types __nan and Null__ values\n",
    "- When we inspected for Null values in the data, there are __no Null values present.__\n",
    "- When we inspected for NaN values in the data, there are __no NaN values present.__\n",
    "- But there are columns which contain values as __'nan' for some rows__. If we treat them as null then __we can filter out these values from dataframe.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">5.2 The Violation Time field is specified in a strange format. Find a way to make this a time attribute that you can use to divide into groups.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Hour, Minute and \"AM/PM\" from Violation_time column to making time attribute\n",
    "df_time = nyc_pt_final.withColumn(\"LeftTime\", func.substring(func.col(\"Violation_Time\"),1, 2).cast(\"int\"))\\\n",
    "                      .withColumn(\"RightTime\", func.substring(func.col(\"Violation_Time\"),3, 2).cast(\"int\"))\\\n",
    "                      .withColumn(\"AMPM\", func.substring(func.col(\"Violation_Time\"),5, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+---------+----+\n",
      "|Violation_Time|LeftTime|RightTime|AMPM|\n",
      "+--------------+--------+---------+----+\n",
      "|         0143A|       1|       43|   A|\n",
      "|         0400P|       4|        0|   P|\n",
      "|         0233P|       2|       33|   P|\n",
      "|         1120A|      11|       20|   A|\n",
      "|         0555P|       5|       55|   P|\n",
      "+--------------+--------+---------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the extracted Hour, Minute and 'AM/PM' column to verify that extraction was successful\n",
    "df_time.select(\"Violation_Time\",\"LeftTime\",\"RightTime\",\"AMPM\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Left24|\n",
      "+------+\n",
      "|     1|\n",
      "|    16|\n",
      "|    14|\n",
      "|    11|\n",
      "|    17|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Converting Hour from 12 Hour format to 24 Hour format\n",
    "df_time2 = df_time.withColumn(\"Left24\", func.when((df_time.AMPM==\"P\") & (df_time.LeftTime < 12), df_time.LeftTime+12)\\\n",
    "                                            .when((df_time.AMPM==\"A\") & (df_time.LeftTime >= 12), df_time.LeftTime-12)\\\n",
    "                                            .otherwise(df_time.LeftTime))\n",
    "\n",
    "df_time2.select(\"Left24\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|Violation_Time| VT24|\n",
      "+--------------+-----+\n",
      "|         0143A| 1:43|\n",
      "|         0400P| 16:0|\n",
      "|         0233P|14:33|\n",
      "|         1120A|11:20|\n",
      "|         0555P|17:55|\n",
      "+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Concatenating Hour and Minute column to make complete time attribute in 24 hour format\n",
    "df_time3 = df_time2.withColumn(\"VT24\", func.concat_ws(\":\",df_time2.Left24, df_time2.RightTime))\n",
    "\n",
    "# Inspecting time in string vs time in 24 hour format\n",
    "df_time3.select(\"Violation_Time\",\"VT24\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">5.3 Divide 24 hours into six equal discrete bins of time. Choose the intervals as you see fit. For each of these groups, find the three most commonly occurring violations.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">Assumption</span>\n",
    "Assuming intervals as follows:\n",
    "  - __0AM - 4AM as Midnight__\n",
    "  - __4AM - 8AM as Early_Morning__\n",
    "  - __8AM - 12PM as Morning__\n",
    "  - __12PM - 16PM (4PM) as After_Noon__\n",
    "  - __16PM (4PM) - 20PM (8PM) as Evening__\n",
    "  - __20PM (8PM) - 24AM(12AM) as Night__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+----------+\n",
      "|Violation_Code| VT24|    VT_BIN|\n",
      "+--------------+-----+----------+\n",
      "|             7| 1:43|  Midnight|\n",
      "|             7| 16:0|   Evening|\n",
      "|             5|14:33|After_Noon|\n",
      "|            47|11:20|   Morning|\n",
      "|            69|17:55|   Evening|\n",
      "+--------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating view to accomodate changes made in dataframe by adding time attribute \n",
    "df_time3.createOrReplaceTempView(\"nyc_with_time\")\n",
    "\n",
    "# Dividing the 24 hour into 6 equal discrete time bin\n",
    "df_time4 = spark.sql('select Violation_Code,VT24,\\\n",
    "                            case \\\n",
    "                              when (Left24>=0 and Left24<4) then \"Midnight\" \\\n",
    "                              when (Left24>=4 and Left24<8) then \"Early_Morning\" \\\n",
    "                              when (Left24>=8 and Left24<12) then \"Morning\" \\\n",
    "                              when (Left24>=12 and Left24<16) then \"After_Noon\" \\\n",
    "                              when (Left24>=16 and Left24<20) then \"Evening\" \\\n",
    "                              else \"Night\" \\\n",
    "                            end as VT_BIN \\\n",
    "                      from nyc_with_time')\n",
    "\n",
    "df_time4.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating view again to accomodate changes made in dataframe by adding discrete time bin\n",
    "df_time4.createOrReplaceTempView(\"nyc_with_intervals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|Violation_Code|Count|\n",
      "+--------------+-----+\n",
      "|            21|77461|\n",
      "|            40|50948|\n",
      "|            78|32243|\n",
      "+--------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding three most commonly occurring violations for Midnight (0AM - 4AM) interval\n",
    "nyc_midnight = spark.sql('select Violation_Code,count(*) as Count\\\n",
    "                          from nyc_with_intervals\\\n",
    "                          where VT_BIN = \"Midnight\" \\\n",
    "                          group by Violation_Code order by Count desc')\n",
    "nyc_midnight.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|Violation_Code| Count|\n",
      "+--------------+------+\n",
      "|            14|141276|\n",
      "|            21|119469|\n",
      "|            40|112187|\n",
      "+--------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding three most commonly occurring violations for Early_Morning (4AM - 8AM) interval\n",
    "nyc_early_morning = spark.sql('select Violation_Code,count(*) as Count\\\n",
    "                               from nyc_with_intervals\\\n",
    "                               where VT_BIN = \"Early_Morning\" \\\n",
    "                               group by Violation_Code order by Count desc')\n",
    "\n",
    "nyc_early_morning.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+\n",
      "|Violation_Code|  Count|\n",
      "+--------------+-------+\n",
      "|            21|1182691|\n",
      "|            36| 751422|\n",
      "|            38| 346518|\n",
      "+--------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding three most commonly occurring violations for Morning (8AM - 12PM) interval\n",
    "nyc_morning = spark.sql('select Violation_Code,count(*) as Count\\\n",
    "                         from nyc_with_intervals\\\n",
    "                         where VT_BIN = \"Morning\" \\\n",
    "                         group by Violation_Code order by Count desc')\n",
    "nyc_morning.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|Violation_Code| Count|\n",
      "+--------------+------+\n",
      "|            36|588395|\n",
      "|            38|462758|\n",
      "|            37|337075|\n",
      "+--------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding three most commonly occurring violations for After_Noon (12PM - 4PM) interval\n",
    "nyc_after_noon = spark.sql('select Violation_Code,count(*) as Count\\\n",
    "                            from nyc_with_intervals\\\n",
    "                            where VT_BIN = \"After_Noon\" \\\n",
    "                            group by Violation_Code order by Count desc')\n",
    "\n",
    "nyc_after_noon.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|Violation_Code| Count|\n",
      "+--------------+------+\n",
      "|            38|203232|\n",
      "|            37|145784|\n",
      "|            14|144749|\n",
      "+--------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding three most commonly occurring violations for Evening (4PM - 8PM) interval\n",
    "nyc_evening = spark.sql('select Violation_Code,count(*) as Count\\\n",
    "                         from nyc_with_intervals\\\n",
    "                         where VT_BIN = \"Evening\" \\\n",
    "                         group by Violation_Code order by Count desc')\n",
    "\n",
    "nyc_evening.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|Violation_Code|Count|\n",
      "+--------------+-----+\n",
      "|             7|65593|\n",
      "|            38|47032|\n",
      "|            14|44787|\n",
      "+--------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding three most commonly occurring violations for Night (8PM - 12AM) interval\n",
    "nyc_night = spark.sql('select Violation_Code,count(*) as Count\\\n",
    "                       from nyc_with_intervals\\\n",
    "                       where VT_BIN = \"Night\" \\\n",
    "                       group by Violation_Code order by Count desc')\n",
    "\n",
    "nyc_night.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">5.4 Now, try another direction. For the three most commonly occurring violation codes, find the most common time of the day (in terms of the bins from the previous part).</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+\n",
      "|Violation_Code|  Count|\n",
      "+--------------+-------+\n",
      "|            21|1528588|\n",
      "|            36|1400614|\n",
      "|            38|1062304|\n",
      "+--------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding three most commonly occuring Violation Code\n",
    "nyc_top3_vc = spark.sql('select Violation_Code,count(*) as Count\\\n",
    "                         from nyc_with_intervals\\\n",
    "                         group by Violation_Code order by Count desc')\n",
    "\n",
    "nyc_top3_vc.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most __commonly occuring Violation Codes are 21, 36 and 38__. Lets find the most common time of the day for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|       VT_BIN|  Count|\n",
      "+-------------+-------+\n",
      "|      Morning|1182691|\n",
      "|   After_Noon| 148014|\n",
      "|Early_Morning| 119469|\n",
      "+-------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identifying most common time of the day when violation code is 21\n",
    "nyc_vc_21 = spark.sql('select VT_BIN, count(*) as Count\\\n",
    "                       from nyc_with_intervals\\\n",
    "                       where Violation_Code = \"21\" \\\n",
    "                       group by VT_BIN order by Count desc')\n",
    "\n",
    "nyc_vc_21.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|       VT_BIN| Count|\n",
      "+-------------+------+\n",
      "|      Morning|751422|\n",
      "|   After_Noon|588395|\n",
      "|Early_Morning| 33939|\n",
      "+-------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identifying most common time of the day when violation code is 36\n",
    "nyc_vc_36 = spark.sql('select VT_BIN, count(*) as Count\\\n",
    "                       from nyc_with_intervals\\\n",
    "                       where Violation_Code = \"36\" \\\n",
    "                       group by VT_BIN order by Count desc')\n",
    "nyc_vc_36.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|    VT_BIN| Count|\n",
      "+----------+------+\n",
      "|After_Noon|462758|\n",
      "|   Morning|346518|\n",
      "|   Evening|203232|\n",
      "+----------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identifying most common time of the day when violation code is 38\n",
    "nyc_vc_38 = spark.sql('select VT_BIN, count(*) as Count\\\n",
    "                       from nyc_with_intervals\\\n",
    "                       where Violation_Code = \"38\" \\\n",
    "                       group by VT_BIN order by Count desc')\n",
    "nyc_vc_38.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "\n",
    "- For Violation_Code - 21, most common time of the day is __Morning (8AM to 12PM)__\n",
    "- For Violation_Code - 36, most common time of the day is __Morning (8AM to 12PM)__\n",
    "- For Violation_Code - 38, most common time of the day is __After_Noon (12PM to 4PM)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">6. Lets try and find some seasonality in this data:</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">6.1 First, divide the year into a certain number of seasons, and find the frequencies of tickets for each season.</span>\n",
    "__(Hint: Use Issue Date to segregate into seasons.)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+-----------+\n",
      "|Summons_Number|Plate_ID|Registration_State|         Issue_Date|Violation_Code|Vehicle_Body_Type|Vehicle_Make|Violation_Precinct|Issuer_Precinct|Violation_Time|Issue_Month|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+-----------+\n",
      "|    5092469481| GZH7067|                NY|2016-07-10 00:00:00|             7|             SUBN|       TOYOT|                 0|              0|         0143A|          7|\n",
      "|    5092451658| GZH7067|                NY|2016-07-08 00:00:00|             7|             SUBN|       TOYOT|                 0|              0|         0400P|          7|\n",
      "|    4006265037| FZX9232|                NY|2016-08-23 00:00:00|             5|             SUBN|        FORD|                 0|              0|         0233P|          8|\n",
      "|    8478629828| 66623ME|                NY|2017-06-14 00:00:00|            47|             REFG|       MITSU|                14|             14|         1120A|          6|\n",
      "|    7868300310| 37033JV|                NY|2016-11-21 00:00:00|            69|             DELV|       INTER|                13|             13|         0555P|         11|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extracting month from the year to divide into seasons\n",
    "mon_df = nyc_pt_final.withColumn(\"Issue_Month\", func.month(nyc_pt_final.Issue_Date))\n",
    "mon_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">Assumption</span>\n",
    "The Seasons have been assumed as :\n",
    "* __Fall Season : September(9), October(10), November(11)__\n",
    "* __Winter Season : December(12), January(1), February(2)__\n",
    "* __Spring Season : March(3), April(4), May(5)__\n",
    "* __Summer Season : June(6), July(7), August(8)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------+\n",
      "|Violation_Code|Issue_Month|Season|\n",
      "+--------------+-----------+------+\n",
      "|             7|          7|Summer|\n",
      "|             7|          7|Summer|\n",
      "|             5|          8|Summer|\n",
      "|            47|          6|Summer|\n",
      "|            69|         11|  Fall|\n",
      "|             7|          6|Summer|\n",
      "|            40|          8|Summer|\n",
      "|            36|         12|Winter|\n",
      "|            36|         11|  Fall|\n",
      "|             5|         10|  Fall|\n",
      "+--------------+-----------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating view to accomodate changes made in dataframe by extracting month from the Issue_Date\n",
    "mon_df.createOrReplaceTempView(\"nyc_with_month\")\n",
    "\n",
    "# Segregating year into 4 seasons\n",
    "df_season = spark.sql('select Violation_Code,Issue_Month,\\\n",
    "                              case \\\n",
    "                                  when (Issue_Month=9 or Issue_Month=10 or Issue_Month=11) then \"Fall\" \\\n",
    "                                  when (Issue_Month=12 or Issue_Month=1 or Issue_Month=2) then \"Winter\" \\\n",
    "                                  when (Issue_Month=3 or Issue_Month=4 or Issue_Month=5) then \"Spring\" \\\n",
    "                                  else \"Summer\" \\\n",
    "                              end as Season \\\n",
    "                       from nyc_with_month')\n",
    "\n",
    "df_season.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To Find the frequency of tickets for each season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|Season|  Count|\n",
      "+------+-------+\n",
      "|Spring|2880687|\n",
      "|  Fall|2830802|\n",
      "|Summer|2606208|\n",
      "|Winter|2485331|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating view to accomodate changes made in dataframe by segregating year into seasons\n",
    "df_season.createOrReplaceTempView(\"nyc_with_season\")\n",
    "\n",
    "# Finding frequency of tickets for each season\n",
    "nyc_ticket_season_freq = spark.sql('select Season, count(*) as Count\\\n",
    "                                    from nyc_with_season\\\n",
    "                                    group by Season order by Count desc')\n",
    "\n",
    "nyc_ticket_season_freq.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">6.2 then, find the three most common violations for each of these seasons.</span>\n",
    "__(Hint: You can use an approach similar to the one mentioned in the hint for question 4.)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|Violation_Code| Count|\n",
      "+--------------+------+\n",
      "|            21|362341|\n",
      "|            36|359338|\n",
      "|            38|259723|\n",
      "+--------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding three most common violations for Winter(Month 12,1,2) season\n",
    "nyc_winter = spark.sql('select Violation_Code, count(*) as Count\\\n",
    "                        from nyc_with_season\\\n",
    "                        where Season=\"Winter\"\\\n",
    "                        group by Violation_Code order by Count desc')\n",
    "\n",
    "nyc_winter.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|Violation_Code| Count|\n",
      "+--------------+------+\n",
      "|            21|402807|\n",
      "|            36|344834|\n",
      "|            38|271192|\n",
      "+--------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding three most common violations for Spring(Month 3,4,5) season\n",
    "nyc_spring = spark.sql('select Violation_Code, count(*) as Count\\\n",
    "                        from nyc_with_season\\\n",
    "                        where Season=\"Spring\"\\\n",
    "                        group by Violation_Code order by Count desc')\n",
    "\n",
    "nyc_spring.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|Violation_Code| Count|\n",
      "+--------------+------+\n",
      "|            21|405961|\n",
      "|            38|247561|\n",
      "|            36|240396|\n",
      "+--------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding three most common violations for Summer(Month 6,7,8) season\n",
    "nyc_summer = spark.sql('select Violation_Code, count(*) as Count\\\n",
    "                        from nyc_with_season\\\n",
    "                        where Season=\"Summer\"\\\n",
    "                        group by Violation_Code order by Count desc')\n",
    "\n",
    "nyc_summer.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|Violation_Code| Count|\n",
      "+--------------+------+\n",
      "|            36|456046|\n",
      "|            21|357479|\n",
      "|            38|283828|\n",
      "+--------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding three most common violations for Fall(Month 9,10,11) season\n",
    "nyc_fall = spark.sql('select Violation_Code, count(*) as Count\\\n",
    "                      from nyc_with_season\\\n",
    "                      where Season=\"Fall\"\\\n",
    "                      group by Violation_Code order by Count desc')\n",
    "\n",
    "nyc_fall.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "\n",
    "- __For Winter(Month 12,1,2) season three most common violations with there frequency are are:__\n",
    "            | Violation Code | No. of Tickets | \n",
    "            |     21         |    362341      | \n",
    "            |     36         |    359338      |\n",
    "            |     38         |    259723      |    \n",
    "\n",
    "- __For Spring(Month 3,4,5) season three most common violations are:__\n",
    "            | Violation Code | No. of Tickets | \n",
    "            |     21         |    402807      |\n",
    "            |     36         |    344834      |\n",
    "            |     38         |    271192      |\n",
    "\n",
    "- __For Summer(Month 6,7,8) season three most common violations are:__\n",
    "            | Violation Code | No. of Tickets | \n",
    "            |     21         |    405961      |\n",
    "            |     38         |    247561      |\n",
    "            |     36         |    240396      |\n",
    "\n",
    "- __For Fall(Month 9,10,11) season three most common violations are:__\n",
    "            | Violation Code | No. of Tickets | \n",
    "            |     36         |    456046      |\n",
    "            |     21         |    357479      |\n",
    "            |     38         |    283828      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">7. The fines collected from all the instances of parking violation constitute a source of revenue for the NYC Police Department. Lets take an example of estimating this for the three most commonly occurring codes:</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">7.1 Find the total occurrences of the three most common violation codes.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+\n",
      "|Violation_Code|  Count|\n",
      "+--------------+-------+\n",
      "|            21|1528588|\n",
      "|            36|1400614|\n",
      "|            38|1062304|\n",
      "+--------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identifying total occurrences of the three most common violation codes\n",
    "Count_df = spark.sql('select Violation_Code, count(*) as Count\\\n",
    "                      from nyc_with_season\\\n",
    "                      group by Violation_Code order by Count desc')\n",
    "\n",
    "Count_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Three most commonly occuring violation codes are 21,36 and 38__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">7.2 then, visit the website:</span>\n",
    "http://www1.nyc.gov/site/finance/vehicles/services-violation-codes.page\n",
    "It lists the fines associated with different violation codes. Theyre divided into two categories: one for the highest-density locations in the city and the other for the rest of the city. __For the sake of simplicity, take the average of the two.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine calculation\n",
    "- __For Violation Code 21__\n",
    "    - Fines associated are highest-density locations in the city - `$65` & other for the rest of the city -`$45`\n",
    "    - Taking average (65+45)/2 we get __fine for violation code 21 is $55__\n",
    "\n",
    "\n",
    "- __For Violation Code 36__\n",
    "    - Fines associated are highest-density locations in the city - `$50` & other for the rest of the city -`$50`\n",
    "    - Taking average (50+50)/2 we get __fine for violation code 36 is $50__\n",
    "    \n",
    "    \n",
    "- __For Violation Code 38__\n",
    "    - Fines associated are highest-density locations in the city - `$65` & other for the rest of the city -`$35`\n",
    "    - Taking average (65+35)/2 we get __fine for violation code 38 is $50__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">7.3 Using this information, find the total amount collected for the three violation codes with the maximum tickets. State the code that has the highest total collection.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating view\n",
    "Count_df.createOrReplaceTempView(\"countTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|  Count|TotalFine|\n",
      "+-------+---------+\n",
      "|1528588| 84072340|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Total Amount collected for Violation Code 21\n",
    "nyc_fine_vc21 = spark.sql('select Count, Count*55 as TotalFine\\\n",
    "                           from countTable\\\n",
    "                           where Violation_Code=\"21\"')\n",
    "\n",
    "nyc_fine_vc21.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|  Count|TotalFine|\n",
      "+-------+---------+\n",
      "|1400614| 70030700|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Total Amount collected for Violation Code 36\n",
    "nyc_fine_vc36 = spark.sql('select Count, Count*50 as TotalFine\\\n",
    "                           from countTable\\\n",
    "                           where Violation_Code=\"36\"')\n",
    "\n",
    "nyc_fine_vc36.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|  Count|TotalFine|\n",
      "+-------+---------+\n",
      "|1062304| 53115200|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Total Amount collected for Violation Code 38\n",
    "nyc_fine_vc38 = spark.sql('select Count, Count*50 as TotalFine\\\n",
    "                           from countTable\\\n",
    "                           where Violation_Code =\"38\"')\n",
    "\n",
    "nyc_fine_vc38.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:red\">7.4 What can you intuitively infer from these findings?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation and Findings\n",
    "Summary for most commonly occuring violation codes i.e. 21,36 and 38\n",
    "\n",
    "| Violation Code | No. of Tickets | Total Fine |\n",
    "| --- | --- | --- |\n",
    "| 21 | 1528588 | 84072340 |\n",
    "| 36 | 1400614 | 70030700 |\n",
    "| 38 | 1062304 | 53115200 |\n",
    "\n",
    "- __Violation Code 21 has the highest Total Collection__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping SparkSession object\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
